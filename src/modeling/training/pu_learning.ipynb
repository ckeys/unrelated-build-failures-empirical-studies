{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:54.222243Z",
     "start_time": "2024-05-23T01:25:54.207759Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def column_rename(df):\n",
    "    '''\n",
    "    Rename the column names\n",
    "    :param df:\n",
    "    :return:\n",
    "    '''\n",
    "    # Dictionary of column name mappings\n",
    "    column_mapping = {\n",
    "        'num_parrel_issues': 'Number of Parallel Issues',\n",
    "        'num_prior_comments': 'Number of Prior Comments',\n",
    "        'if_cross_project': 'Is Cross Projects',\n",
    "        'type_id': 'Type Id',\n",
    "        'priority_id': 'Priority',\n",
    "        'num_of_sf_comments': 'Number of Similar Failures',\n",
    "        'share_same_emsg': 'Is Shared Same Emsg',\n",
    "        'has_config_files': 'Has Config Files',\n",
    "        'has_source_code': 'Has Source Code',\n",
    "        'lines_of_source_code_added': 'Source Code Lines Added',\n",
    "        'lines_of_source_code_deleted': 'Source Code Lines Deleted',\n",
    "        'lines_of_source_code_modified': 'Source Code Lines Modified',\n",
    "        'num_of_modified_source_code_files': 'Modified Source Code Files',\n",
    "        'lines_of_config_file_added': 'Config Lines Added',\n",
    "        'lines_of_config_file_deleted': 'Config Lines Deleted',\n",
    "        'lines_of_config_file_modified': 'Config Lines Modified',\n",
    "        'num_of_modified_config_files': 'Modified Config Files',\n",
    "        'has_contains_code_patch': 'Has Code Patch',\n",
    "        'lines_of_Test_classes_added': 'Test Classes Added',\n",
    "        'lines_of_Test_classes_deleted': 'Test Classes Deleted',\n",
    "        'lines_of_Test_classes_modified': 'Test Classes Lines Modified',\n",
    "        'lines_of_Test_files': 'Modified Test Classes Files',\n",
    "        'has_contain_Test_files': 'Has Test Files',\n",
    "        'gap_days': 'CI Latency',\n",
    "        'is_daily_time': 'Daily Time',\n",
    "        'is_night_time': 'Night Time',\n",
    "        'is_weekday': 'Weekday',\n",
    "        'is_weekend': 'Weekend'\n",
    "    }\n",
    "    # Rename columns using the dictionary\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    # Loop through columns and rename if they start with \"is_\"\n",
    "    for column in df.columns:\n",
    "        if column.startswith('is_'):\n",
    "            new_column = \"Is \" + column[3:]\n",
    "            df.rename(columns={column: new_column}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_selection(project, df):\n",
    "    '''\n",
    "    This is a function that only return the df with features and remove unsed features.\n",
    "    The Feature Selection is done by redn and varclus analysis by R.\n",
    "    We also public the R script in the repo\n",
    "    :param project:\n",
    "    :param df:\n",
    "    :return:\n",
    "    '''\n",
    "    selection_map = {\n",
    "        \"ambari\": [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Is Cross Projects\",\n",
    "                   \"Number of Similar Failures\", \"Is Reference\", \"Is Duplicate\", \"Is Regression\", \"Is Blocker\",\n",
    "                   \"Is Container\", \"Is dependent\", \"Is Required\", \"Is Cloners\", \"Is Blocked\", \"Config Lines Modified\",\n",
    "                   \"Has Code Patch\", \"CI Latency\", \"Night Time\", \"Weekend\"],\n",
    "        'hadoop': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                   \"Is Shared Same Emsg\", \"Is dependent\", \"Is Duplicate\", \"Is Reference\", \"Is Cloners\", \"Is Regression\",\n",
    "                   \"Is Blocker\", \"Is Supercedes\", \"Is Container\", \"Is Required\", \"Is Problem/Incident\", \"Is Blocked\",\n",
    "                   \"Is Dependent\", \"Is Completes\", \"Is Child-Issue\", \"Config Lines Modified\", \"Has Code Patch\",\n",
    "                   \"CI Latency\", \"Night Time\", \"Weekend\"],\n",
    "        'hbase': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                  \"Is Shared Same Emsg\", \"Is Incorporates\", \"Is Reference\", \"Is dependent\", \"Is Duplicate\",\n",
    "                  \"Is Supercedes\", \"Is Cloners\", \"Is Regression\", \"Is Required\", \"Is Dependent\", \"Is Container\",\n",
    "                  \"Is Child-Issue\", \"Is Problem/Incident\", \"Is Blocked\", \"Is Completes\", \"Config Lines Modified\",\n",
    "                  \"Has Code Patch\", \"CI Latency\", \"Night Time\", \"Weekend\"],\n",
    "        'hdds': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                 \"Is Shared Same Emsg\", \"Is Duplicate\", \"Is Reference\", \"Is Blocker\", \"Is Cloners\", \"Is Child-Issue\",\n",
    "                 \"Is dependent\", \"Is Problem/Incident\", \"Is Regression\", \"Is Container\", \"Is Dependent\",\n",
    "                 \"Is Supercedes\", \"Is Required\", \"Is Blocked\", \"Is Dependency\", \"Config Lines Deleted\",\n",
    "                 \"Config Lines Modified\", \"Has Code Patch\", \"CI Latency\", \"Night Time\", \"Weekend\"],\n",
    "        'hdfs': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                 \"Is Shared Same Emsg\", \"Is Reference\", \"Is Incorporates\", \"Is Blocker\", \"Is Container\", \"Is dependent\",\n",
    "                 \"Is Supercedes\", \"Is Regression\", \"Is Cloners\", \"Is Required\", \"Is Problem/Incident\", \"Is Dependent\",\n",
    "                 \"Is Child-Issue\", \"Is Blocked\", \"Is Completes\", \"Is Parent Feature\", \"Is Dependency\",\n",
    "                 \"Config Lines Deleted\", \"Config Lines Modified\", \"Has Code Patch\", \"CI Latency\", \"Night Time\",\n",
    "                 \"Weekend\"],\n",
    "        'hive': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                 \"Is Shared Same Emsg\", \"Is dependent\", \"Is Blocker\", \"Is Reference\", \"Is Incorporates\",\n",
    "                 \"Is Regression\", \"Is Cloners\", \"Is Child-Issue\", \"Is Required\", \"Is Supercedes\", \"Is Container\",\n",
    "                 \"Is Problem/Incident\", \"Is Blocked\", \"Is Completes\", \"Is Dependent\", \"Is Parent Feature\",\n",
    "                 \"Is Dependency\", \"Config Lines Modified\", \"Has Code Patch\", \"CI Latency\", \"Night Time\", \"Weekend\"],\n",
    "        'yarn': [\"Number of Parallel Issues\", \"Number of Prior Comments\", \"Number of Similar Failures\",\n",
    "                 \"Is Shared Same Emsg\", \"Is Incorporates\", \"Is Reference\", \"Is dependent\", \"Is Duplicate\",\n",
    "                 \"Is Regression\", \"Is Cloners\", \"Is Required\", \"Is Supercedes\", \"Is Container\", \"Is Blocked\",\n",
    "                 \"Is Problem/Incident\", \"Is Child-Issue\", \"Is Dependency\", \"Is Dependent\", \"Is Completes\",\n",
    "                 \"Config Lines Deleted\", \"Config Lines Modified\", \"Has Code Patch\", \"CI Latency\", \"Night Time\",\n",
    "                 \"Weekend\"]\n",
    "    }\n",
    "    selected_columns = ['project_name', 'comment_id', 'issue_id','label','comment_created_at_ts'] + selection_map[project]\n",
    "    return df[selected_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:54.236303Z",
     "start_time": "2024-05-23T01:25:54.225202Z"
    }
   },
   "id": "4d9a94bdbdd531cb",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "current_file_path = os.getcwd()\n",
    "base_path = '/'.join(current_file_path.split('/')[:-3] + ['data', 'modeling_data','training'])\n",
    "project_name_group = ['hive', 'hadoop', 'yarn', 'hdfs', 'hbase', 'ambari', 'hdds']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:54.237709Z",
     "start_time": "2024-05-23T01:25:54.235647Z"
    }
   },
   "id": "ac936dde297bf2b6",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "project_name = project_name_group[0]\n",
    "positive_data = pd.read_csv(f'''{base_path}/labeled_data/{project_name}_labeled_data.csv''')\n",
    "positive_data = positive_data[['issue_id', 'comment_id']]\n",
    "# Set labels in benchmark_data to 1\n",
    "positive_data['label'] = 1\n",
    "# Get unique 'issue_id' and 'comment_id' in benchmark_data\n",
    "positive_ids = positive_data[['issue_id', 'comment_id']].values\n",
    "data_with_features = pd.read_csv(f'''{base_path}/features/{project_name}_data_with_features.csv''')\n",
    "data_with_features['issue_id'] = data_with_features['project_name'].str.cat(data_with_features['issue_id'].astype(str), sep='-')\n",
    "data_with_features['label'] = np.nan\n",
    "df = data_with_features.merge(positive_data, on=['issue_id', 'comment_id'], how='left', suffixes=('_feature_df', '_positive_df'))\n",
    "df['label'] = df.apply(lambda row: 1 if row['label_positive_df'] == 1 else -1, axis=1)\n",
    "# Getting the distribution of the 'label' column\n",
    "label_distribution = df['label'].value_counts()\n",
    "\n",
    "# print(label_distribution)\n",
    "df.drop(['label_feature_df'], axis=1, inplace=True)\n",
    "df.drop('label_positive_df', axis=1, inplace=True)\n",
    "# print(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:54.436852Z",
     "start_time": "2024-05-23T01:25:54.241327Z"
    }
   },
   "id": "a79a548b695a277a",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adjust the value of has_source_code column based on conditions\n",
    "df.loc[(df['lines_of_source_code_added'] != 0) |\n",
    "       (df['lines_of_source_code_deleted'] != 0) |\n",
    "       (df['lines_of_source_code_modified'] != 0), 'has_source_code'] = 1\n",
    "df.loc[(df['lines_of_source_code_added'] == 0) &\n",
    "       (df['lines_of_source_code_deleted'] == 0) &\n",
    "       (df['lines_of_source_code_modified'] == 0), 'has_source_code'] = 0\n",
    "\n",
    "df.loc[(df['lines_of_config_file_added'] != 0) |\n",
    "       (df['lines_of_config_file_deleted'] != 0) |\n",
    "       (df['lines_of_config_file_modified'] != 0), 'has_config_files'] = 1\n",
    "df.loc[(df['lines_of_config_file_added'] == 0) &\n",
    "       (df['lines_of_config_file_deleted'] == 0) &\n",
    "       (df['lines_of_config_file_modified'] == 0), 'has_config_files'] = 0\n",
    "\n",
    "# Set the value of has_contains_code_patch based on conditions\n",
    "df.loc[(df['has_config_files'] == 1) | (df['has_source_code'] == 1), 'has_contains_code_patch'] = 1\n",
    "df.loc[(df['has_config_files'] != 1) & (df['has_source_code'] != 1), 'has_contains_code_patch'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:54.443316Z",
     "start_time": "2024-05-23T01:25:54.440433Z"
    }
   },
   "id": "cd15ab26a2141354",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert comment_created_at to datetime\n",
    "df['comment_created_at'] = pd.to_datetime(df['comment_created_at'], format='%a %d %b %Y %H:%M:%S %z')\n",
    "df['comment_created_at_str'] = df['comment_created_at'].astype(str)\n",
    "df['comment_created_at_ts'] = pd.Series(dtype='int64')\n",
    "for i, value in enumerate(df['comment_created_at_str']):\n",
    "    timestamp = pd.Timestamp(value)\n",
    "    ts_int = int(timestamp.timestamp())\n",
    "    df.at[i, 'comment_created_at_ts'] = ts_int\n",
    "df = df.drop(columns=['comment_created_at_str'])\n",
    "df['created'] = pd.to_datetime(df['created'], format='%a %d %b %Y %H:%M:%S %z')\n",
    "# df['comment_created_at'] = pd.to_timestamp(df['comment_created_at'], format='%a %d %b %Y %H:%M:%S %z')\n",
    "df['created_str'] = df['created'].astype(str)\n",
    "df['created_ts'] = pd.Series(dtype='int64')\n",
    "for i, value in enumerate(df['created_str']):\n",
    "    timestamp = pd.Timestamp(value)\n",
    "    ts_int = int(timestamp.timestamp())\n",
    "    df.at[i, 'created_ts'] = ts_int\n",
    "df = df.drop(columns=['created_str'])\n",
    "# df['created'] = pd.to_datetime(df['created'], unit='s',utc=True).dt.tz_convert('UTC')\n",
    "df['gap_days'] = (df['comment_created_at_ts'] - df['created_ts']) / 86400\n",
    "df['comment_created_at'] = pd.to_datetime(df['comment_created_at'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "df['hour'] = df['comment_created_at'].dt.strftime('%H').astype(int)\n",
    "# Determine if it's daily time (between 6 AM and 6 PM)\n",
    "df['is_daily_time'] = df['hour'].between(6, 18).astype(int)\n",
    "\n",
    "# Determine if it's night time (between 6 PM and 6 AM)\n",
    "# df['is_night_time'] = (~df['is_daily_time']).astype(int)\n",
    "df['is_night_time'] = (df['hour'] >= 19) | (df['hour'] < 6)\n",
    "df['is_night_time'] = df['is_night_time'].astype(int)\n",
    "\n",
    "# Determine if it's a weekday (Monday=0, Sunday=6)\n",
    "df['weekday'] = df['comment_created_at'].dt.weekday\n",
    "df['is_weekday'] = df['weekday'].isin(range(0, 5)).astype(int)\n",
    "\n",
    "# Determine if it's a weekend (Saturday or Sunday)\n",
    "df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop the intermediate columns if needed\n",
    "df = df.drop(['hour', 'weekday'], axis=1)\n",
    "columns_to_drop = ['type_id', 'priority_id', 'has_contain_Test_files', 'lines_of_Test_classes_modified',\n",
    "                       'lines_of_Test_classes_added', 'lines_of_Test_classes_deleted', 'lines_of_Test_files',\n",
    "                       'deletions', 'insertions', 'files', 'Time_difference', 'commit_num', 'is_share_similar_emsg',\n",
    "                       'is_share_same_emsg', 'num_parrel_commits', 'num_of_developers','created', 'created_ts', 'comment_created_at', 'comment_created_at_ts', 'time', 'day']\n",
    "\n",
    "columns_to_drop = ['type_id', 'priority_id', 'has_contain_Test_files', 'lines_of_Test_classes_modified',\n",
    "                       'lines_of_Test_classes_added', 'lines_of_Test_classes_deleted', 'lines_of_Test_files',\n",
    "                       'deletions', 'insertions', 'files', 'Time_difference', 'commit_num', 'is_share_similar_emsg',\n",
    "                       'is_share_same_emsg', 'num_parrel_commits', 'num_of_developers','created', 'created_ts', 'comment_created_at', 'time', 'day']\n",
    "\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "# List of columns to exclude from filling NaN values with 0\n",
    "columns_to_exclude = ['label']\n",
    "# Fill NaN values with -1 in all columns except 'label'\n",
    "df.loc[:, ~df.columns.isin(columns_to_exclude)] = df.loc[:, ~df.columns.isin(columns_to_exclude)].fillna(0)\n",
    "\n",
    "df = column_rename(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:55.010993Z",
     "start_time": "2024-05-23T01:25:54.496400Z"
    }
   },
   "id": "2db751c4d39efcd0",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "-1    23786\n",
      " 1       99\n",
      "Name: count, dtype: int64\n",
      "Index(['project_name', 'comment_id', 'issue_id', 'Number of Parallel Issues',\n",
      "       'Number of Prior Comments', 'Is Cross Projects',\n",
      "       'Number of Similar Failures', 'Is Shared Same Emsg', 'Is Duplicate',\n",
      "       'Is dependent', 'Is Blocker', 'Is Reference', 'Is Incorporates',\n",
      "       'Is Regression', 'Is Cloners', 'Is Child-Issue', 'Is Required',\n",
      "       'Is Supercedes', 'Is Container', 'Is Problem/Incident', 'Is Blocked',\n",
      "       'Is Completes', 'Is Dependent', 'Is Parent Feature', 'Is Dependency',\n",
      "       'Has Config Files', 'Has Source Code', 'Source Code Lines Added',\n",
      "       'Source Code Lines Deleted', 'Source Code Lines Modified',\n",
      "       'Modified Source Code Files', 'Config Lines Added',\n",
      "       'Config Lines Deleted', 'Config Lines Modified',\n",
      "       'Modified Config Files', 'Has Code Patch', 'label',\n",
      "       'comment_created_at_ts', 'CI Latency', 'Daily Time', 'Night Time',\n",
      "       'Weekday', 'Weekend'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n",
    "print(df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T01:25:55.015605Z",
     "start_time": "2024-05-23T01:25:55.011924Z"
    }
   },
   "id": "bdbb07b6adf39935",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PU Learning with LOOCV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dc55b81387252fa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from pulearn import ElkanotoPuClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T02:55:59.626141Z",
     "start_time": "2024-05-23T02:55:59.624193Z"
    }
   },
   "id": "4730fc12b2d4afe3",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['project_name', 'comment_id', 'issue_id', 'label',\n",
      "       'comment_created_at_ts', 'Number of Parallel Issues',\n",
      "       'Number of Prior Comments', 'Number of Similar Failures',\n",
      "       'Is Shared Same Emsg', 'Is dependent', 'Is Blocker', 'Is Reference',\n",
      "       'Is Incorporates', 'Is Regression', 'Is Cloners', 'Is Child-Issue',\n",
      "       'Is Required', 'Is Supercedes', 'Is Container', 'Is Problem/Incident',\n",
      "       'Is Blocked', 'Is Completes', 'Is Dependent', 'Is Parent Feature',\n",
      "       'Is Dependency', 'Config Lines Modified', 'Has Code Patch',\n",
      "       'CI Latency', 'Night Time', 'Weekend'],\n",
      "      dtype='object')\n",
      "This is the distribution of label:(array([-1,  1]), array([98, 98]))\n",
      "(196, 25),(196,)\n",
      "holdout is [156 189  78 154 115 121 167  70 117 136  75 111 179 180  64  48 129  91\n",
      " 188 106] and len(X_p_hold_out) is 13\n",
      "HIVE feature issue_id score is 0.21694773637053022\n",
      "HIVE feature comment_created_at_ts score is 0.1916254736782342\n",
      "HIVE feature Is Parent Feature score is 0.1788349360496518\n",
      "HIVE feature comment_id score is 0.1121150143770588\n",
      "HIVE feature Is Completes score is 0.06201207033264117\n",
      "HIVE feature Config Lines Modified score is 0.04376753553860868\n",
      "HIVE feature Number of Similar Failures score is 0.036312448223474784\n",
      "HIVE feature Is Dependency score is 0.03421283480030347\n",
      "HIVE feature project_name score is 0.02913308164595277\n",
      "HIVE feature Is Dependent score is 0.02252050596253208\n",
      "HIVE feature Number of Prior Comments score is 0.0181358095160321\n",
      "HIVE feature Is Incorporates score is 0.013364061284354835\n",
      "HIVE feature Is dependent score is 0.012538949012634663\n",
      "HIVE feature Number of Parallel Issues score is 0.007527106746564776\n",
      "HIVE feature Is Blocker score is 0.005756390790596422\n",
      "HIVE feature Is Shared Same Emsg score is 0.004181389005588865\n",
      "HIVE feature Is Child-Issue score is 0.0025981379125350383\n",
      "HIVE feature Is Cloners score is 0.0021120612943687205\n",
      "HIVE feature Is Supercedes score is 0.002076314451557414\n",
      "HIVE feature Is Regression score is 0.0019761733570313514\n",
      "HIVE feature Is Reference score is 0.0016025700438835728\n",
      "HIVE feature Is Required score is 0.00064939960586431\n",
      "HIVE feature Is Container score is 0.0\n",
      "HIVE feature Is Problem/Incident score is 0.0\n",
      "HIVE feature Is Blocked score is 0.0\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "recall\n",
      "HIVE Config Lines Modified 0.000 +/- 0.000\n",
      "HIVE Is Incorporates 0.000 +/- 0.000\n",
      "HIVE comment_id 0.000 +/- 0.000\n",
      "HIVE issue_id 0.000 +/- 0.000\n",
      "HIVE comment_created_at_ts 0.000 +/- 0.000\n",
      "HIVE Number of Parallel Issues 0.000 +/- 0.000\n",
      "HIVE Number of Prior Comments 0.000 +/- 0.000\n",
      "HIVE Number of Similar Failures 0.000 +/- 0.000\n",
      "HIVE Is Shared Same Emsg 0.000 +/- 0.000\n",
      "HIVE Is dependent 0.000 +/- 0.000\n",
      "HIVE Is Blocker 0.000 +/- 0.000\n",
      "HIVE Is Reference 0.000 +/- 0.000\n",
      "HIVE Is Regression 0.000 +/- 0.000\n",
      "HIVE Is Dependency 0.000 +/- 0.000\n",
      "HIVE Is Cloners 0.000 +/- 0.000\n",
      "HIVE Is Child-Issue 0.000 +/- 0.000\n",
      "HIVE Is Required 0.000 +/- 0.000\n",
      "HIVE Is Supercedes 0.000 +/- 0.000\n",
      "HIVE Is Container 0.000 +/- 0.000\n",
      "HIVE Is Problem/Incident 0.000 +/- 0.000\n",
      "HIVE Is Blocked 0.000 +/- 0.000\n",
      "HIVE Is Completes 0.000 +/- 0.000\n",
      "HIVE Is Dependent 0.000 +/- 0.000\n",
      "HIVE Is Parent Feature 0.000 +/- 0.000\n",
      "HIVE project_name 0.000 +/- 0.000\n",
      "f1\n",
      "HIVE Config Lines Modified 0.000 +/- 0.000\n",
      "HIVE Is Incorporates 0.000 +/- 0.000\n",
      "HIVE comment_id 0.000 +/- 0.000\n",
      "HIVE issue_id 0.000 +/- 0.000\n",
      "HIVE comment_created_at_ts 0.000 +/- 0.000\n",
      "HIVE Number of Parallel Issues 0.000 +/- 0.000\n",
      "HIVE Number of Prior Comments 0.000 +/- 0.000\n",
      "HIVE Number of Similar Failures 0.000 +/- 0.000\n",
      "HIVE Is Shared Same Emsg 0.000 +/- 0.000\n",
      "HIVE Is dependent 0.000 +/- 0.000\n",
      "HIVE Is Blocker 0.000 +/- 0.000\n",
      "HIVE Is Reference 0.000 +/- 0.000\n",
      "HIVE Is Regression 0.000 +/- 0.000\n",
      "HIVE Is Dependency 0.000 +/- 0.000\n",
      "HIVE Is Cloners 0.000 +/- 0.000\n",
      "HIVE Is Child-Issue 0.000 +/- 0.000\n",
      "HIVE Is Required 0.000 +/- 0.000\n",
      "HIVE Is Supercedes 0.000 +/- 0.000\n",
      "HIVE Is Container 0.000 +/- 0.000\n",
      "HIVE Is Problem/Incident 0.000 +/- 0.000\n",
      "HIVE Is Blocked 0.000 +/- 0.000\n",
      "HIVE Is Completes 0.000 +/- 0.000\n",
      "HIVE Is Dependent 0.000 +/- 0.000\n",
      "HIVE Is Parent Feature 0.000 +/- 0.000\n",
      "HIVE project_name 0.000 +/- 0.000\n",
      "HIVE at iteration 1 PULearning -- precision:1.0, recall:1.0, f1:1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'HIVE'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/f5/yg7slzsx58n3bqzll3cjygjh0000gn/T/ipykernel_5139/136089566.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[0mscoring\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'recall'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'f1'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m         r_multi = permutation_importance(pu_estimator.estimator, X_pred, np.ones(len(X_pred)), n_repeats=10,\n\u001B[1;32m    126\u001B[0m                                          \u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m                                          scoring=scoring)\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mmetric\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mr_multi\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{metric}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr_multi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmetric\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"importances_mean\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1147\u001B[0m                 skip_parameter_validation=(\n\u001B[1;32m   1148\u001B[0m                     \u001B[0mprefer_skip_nested_validation\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mglobal_skip_validation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m                 )\n\u001B[1;32m   1150\u001B[0m             ):\n\u001B[0;32m-> 1151\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfit_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    344\u001B[0m         \"\"\"\n\u001B[1;32m    345\u001B[0m         \u001B[0;31m# Validate or convert input data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    347\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 348\u001B[0;31m         X, y = self._validate_data(\n\u001B[0m\u001B[1;32m    349\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmulti_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"csc\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mDTYPE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    350\u001B[0m         )\n\u001B[1;32m    351\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msample_weight\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    617\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;34m\"estimator\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcheck_y_params\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    618\u001B[0m                     \u001B[0mcheck_y_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mdefault_check_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    619\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"y\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    620\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 621\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    622\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    623\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    624\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mcheck_params\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ensure_2d\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1143\u001B[0m         raise ValueError(\n\u001B[1;32m   1144\u001B[0m             \u001B[0;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1145\u001B[0m         )\n\u001B[1;32m   1146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m     X = check_array(\n\u001B[0m\u001B[1;32m   1148\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m         \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1150\u001B[0m         \u001B[0maccept_large_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_large_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    915\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mxp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_asarray_with_order\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mxp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    918\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 919\u001B[0;31m                 raise ValueError(\n\u001B[0m\u001B[1;32m    920\u001B[0m                     \u001B[0;34m\"Complex data not supported\\n{}\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    921\u001B[0m                 ) from complex_warning\n\u001B[1;32m    922\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/sklearn/utils/_array_api.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(array, dtype, order, copy, xp)\u001B[0m\n\u001B[1;32m    376\u001B[0m         \u001B[0;31m# Use NumPy API to support order\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcopy\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m             \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    379\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 380\u001B[0;31m             \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m         \u001B[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m         \u001B[0;31m# container that is consistent with the input's namespace.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/OtagoPhd/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m   1996\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__array__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnpt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDTypeLike\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1997\u001B[0m         \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1998\u001B[0;31m         \u001B[0marr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1999\u001B[0m         if (\n\u001B[1;32m   2000\u001B[0m             \u001B[0mastype_is_view\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2001\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0musing_copy_on_write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'HIVE'"
     ]
    }
   ],
   "source": [
    "project_name = 'hive' \n",
    "# Feature selection function call (implementation not shown)\n",
    "df = feature_selection(project_name, df)\n",
    "prediction_df = df\n",
    "\n",
    "# Extract the integer part of the issue_id and sort by it and comment_id\n",
    "df = df.sort_values(by=['comment_created_at_ts'], ascending=True).reset_index(drop=True)\n",
    "print(df.columns)\n",
    "# Split data into positive (labeled) and unlabeled portions\n",
    "positive_data = df[df[\"label\"] == 1].reset_index(drop=True)\n",
    "df_for_prediction = df.drop(\"label\", axis=1)\n",
    "unlabeled_data = df[df[\"label\"].isnull()]\n",
    "num_positive_samples = len(df[df['label'] == 1])\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "random.seed(42)\n",
    "train_data = positive_data.copy().drop(\"label\", axis=1)\n",
    "val_data = pd.DataFrame()\n",
    "columns = train_data.columns.tolist()\n",
    "# Performance metrics\n",
    "precision_pu, recall_pu, f1_pu, auc_pu = [], [], [], []\n",
    "precision_rf, recall_rf, f1_rf, auc_rf = [], [], [],[]\n",
    "model_performance_data = []\n",
    "\n",
    "best_model_recall = 0\n",
    "best_model = None\n",
    "best_model_importance = pd.DataFrame(columns=['feature_name', 'importance_mean', 'importance_std'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LOOCV implementation\n",
    "for i in range(1, num_positive_samples):\n",
    "    # Select the last `i` positive samples as testing data\n",
    "    test_positive_data = positive_data.iloc[-i:]\n",
    "    # Select the first `num_pos - i` positive samples for training\n",
    "    train_positive_data = positive_data.iloc[:num_positive_samples - i]\n",
    "    # Select `num_pos - i` negative samples where the order is before the testing dataset\n",
    "    train_negative_data = df[(df['label']==-1) & \n",
    "                             (df['comment_created_at_ts'] < test_positive_data['comment_created_at_ts'].min())]\n",
    "    \n",
    "    for j in range(100):\n",
    "        precision_pu_round,recall_pu_round,f1_pu_round,auc_pu_round = [],[],[],[]\n",
    "        precision_rf_round,recall_rf_round,f1_rf_round,auc_rf_round = [],[],[],[]\n",
    "        \n",
    "        train_negative_sample = train_negative_data.sample(num_positive_samples - i, random_state=42)\n",
    "        # Combine training data\n",
    "        pu_data = pd.concat([train_positive_data, train_negative_sample])\n",
    "        # Extract the 'label' column as a NumPy array\n",
    "        pu_labels = pu_data['label'].to_numpy()\n",
    "        pu_data_with_features_only = pu_data.drop(columns=['project_name', 'comment_id', 'issue_id', 'comment_created_at_ts','label'])\n",
    "        print(f'''This is the distribution of label:{np.unique(pu_labels, return_counts=True)}''')\n",
    "        pu_combined = np.column_stack((pu_data_with_features_only, pu_labels))\n",
    "        np.random.shuffle(pu_combined)\n",
    "        # Separate the shuffled data and labels\n",
    "        pu_data_with_features_only = pu_combined[:, :-1]\n",
    "        pu_labels = pu_combined[:, -1].astype(int)\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        estimator = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            bootstrap=True,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        pu_estimator = ElkanotoPuClassifier(estimator=estimator, hold_out_ratio=0.1)\n",
    "        print(f'''{pu_data_with_features_only.shape},{pu_labels.shape}''')\n",
    "        pu_estimator.fit(pu_data_with_features_only, pu_labels)\n",
    "        importance = pu_estimator.estimator.feature_importances_\n",
    "        combined_dict = dict(zip(columns, importance))\n",
    "        sorted_dict = dict(sorted(combined_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        for k, v in sorted_dict.items():\n",
    "            print(f'''{project_name.upper()} feature {k} score is {v}''')\n",
    "       \n",
    "        test_data_only_with_features = test_positive_data.drop(columns=['project_name', 'comment_id', 'issue_id', 'comment_created_at_ts','label'])\n",
    "        X_pred = test_data_only_with_features.values\n",
    "        y_pred = pu_estimator.predict(X_pred)\n",
    "        precision = precision_score(np.ones(len(X_pred)), y_pred, zero_division=1)\n",
    "        recall = recall_score(np.ones(len(X_pred)), y_pred, zero_division=1)\n",
    "        f1 = f1_score(np.ones(len(X_pred)), y_pred, zero_division=1)\n",
    "        print(type(np.ones(len(X_pred))))\n",
    "        print(type(y_pred))\n",
    "        # auc_score = roc_auc_score(np.ones(len(X_pred), y_pred))\n",
    "        \n",
    "        round_results_pu = {\n",
    "            \"dropout\": i,\n",
    "            \"f1\": f1,\n",
    "            \"recall\": recall,\n",
    "            \"model\": \"PULearning\",\n",
    "            \"iteration\": j\n",
    "        }\n",
    "        best_model = pu_estimator\n",
    "        feature_importance_list = []\n",
    "        scoring = ['recall', 'f1']\n",
    "        r_multi = permutation_importance(pu_estimator.estimator, X_pred, np.ones(len(X_pred)), n_repeats=10,\n",
    "                                         random_state=0,\n",
    "                                         scoring=scoring)\n",
    "        for metric in r_multi:\n",
    "            print(f'{metric}')\n",
    "            r = r_multi[metric]\n",
    "            for j in r[\"importances_mean\"].argsort()[::-1]:\n",
    "                # if r[\"importances_mean\"][i] - 2 * r[\"importances_std\"][i] > 0:\n",
    "                feature_dict = {\n",
    "                    \"feature_name\": columns[j],  # Replace with your feature names\n",
    "                    \"importance_mean\": r[\"importances_mean\"][j],\n",
    "                    \"importance_std\": r[\"importances_std\"][j],\n",
    "                    \"metric\": metric,\n",
    "                }\n",
    "                print(\n",
    "                    f'''{project_name.upper()} {columns[j]:<8} {r[\"importances_mean\"][j]:.3f} +/- {r[\"importances_std\"][j]:.3f}''')\n",
    "                feature_importance_list.append(feature_dict)\n",
    "            new_df = pd.DataFrame(feature_importance_list)\n",
    "            best_model_importance = pd.concat([best_model_importance, new_df], ignore_index=True)\n",
    "        model_performance_data.append(round_results_pu)\n",
    "        # results_df = results_df.append(round_results_pu, ignore_index=True)\n",
    "        print(\n",
    "            f'''{project_name.upper()} at iteration {i} PULearning -- precision:{precision}, recall:{recall}, f1:{f1}''')\n",
    "        precision_pu_round.append(precision)\n",
    "        recall_pu_round.append(recall)\n",
    "        f1_pu_round.append(f1)\n",
    "        # auc_pu_round.append(auc_score)\n",
    "        \n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        newRF = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            bootstrap=True,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        newRF.fit(pu_data, pu_labels)\n",
    "        X_pred = test_data_only_with_features.values\n",
    "        y_pred = newRF.predict(X_pred)\n",
    "        precision = precision_score(np.ones(len(test_data_only_with_features)), y_pred, zero_division=1)\n",
    "        recall = recall_score(np.ones(len(test_data_only_with_features)), y_pred, zero_division=1)\n",
    "        f1 = f1_score(np.ones(len(test_data_only_with_features)), y_pred, zero_division=1)\n",
    "        # auc = roc_auc_score(np.ones(len(test_data_only_with_features)), y_pred, zero_division=1)\n",
    "        \n",
    "        precision_rf_round.append(precision)\n",
    "        recall_rf_round.append(recall)\n",
    "        f1_rf_round.append(f1)\n",
    "        round_results_rf = {\n",
    "            \"dropout\": i,\n",
    "            \"f1\": f1,\n",
    "            \"recall\": recall,\n",
    "            \"model\": \"RandomForest\",\n",
    "            \"iteration\": j\n",
    "        }\n",
    "        model_performance_data.append(round_results_rf)\n",
    "        print(f'''{project_name.upper()} Normal Random Forest -- precision:{precision}, recall:{recall}, f1:{f1}''')\n",
    "        train_data = positive_data.copy()\n",
    "        test_data_only_with_features = pd.DataFrame()\n",
    "        train_data = train_data.drop(\"label\", axis=1)\n",
    "        \n",
    "    precision_pu.append(np.mean(precision_pu_round))\n",
    "    recall_pu.append(np.mean(recall_pu_round))\n",
    "    f1_pu.append(np.mean(f1_pu_round))\n",
    "    \n",
    "    precision_rf.append(np.mean(precision_rf_round))\n",
    "    recall_rf.append(np.mean(recall_rf_round))\n",
    "    f1_rf.append(np.mean(f1_rf_round))\n",
    "\n",
    "X_pred = df_for_prediction.values\n",
    "y_pred = best_model.predict(X_pred)\n",
    "prediction_df[\"new_label\"] = y_pred\n",
    "print('/'.join(base_path.split(\"/\")[:-1] + ['prediction_with_LOOCV_results', f'''{project_name}_prediction.csv''']))\n",
    "prediction_df.to_csv('/'.join(base_path.split(\"/\")[:-1] + ['prediction_with_LOOCV_results', f'''{project_name}_prediction.csv''']),\n",
    "                     index=False)\n",
    "results_df = pd.DataFrame(model_performance_data)\n",
    "print(f\"{project_name.upper()} Writting to {base_path}/training_with_LOOCV_performance/{project_name}_model_performance.csv\")\n",
    "results_df.to_csv(f'''{base_path}/training_with_LOOCV_performance/{project_name}_model_performance.csv''',\n",
    "                  index=False)\n",
    "best_model_importance.to_csv(f'''{base_path}/training_with_LOOCV_performance/{project_name}_model_importance.csv''', index=False)\n",
    "    \n",
    "    # print(pu_combined)\n",
    "    # break\n",
    "    # print(\"Training data\")\n",
    "    # print(pu_data[['issue_id', 'comment_id', 'label', 'issue_id_int']])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T03:10:33.026216Z",
     "start_time": "2024-05-23T03:10:32.384562Z"
    }
   },
   "id": "4119ba895e8a6bcc",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e630284101e1ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
